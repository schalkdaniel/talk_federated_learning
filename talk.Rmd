---
title: Evaluation of Distributed Computing Frameworks
subtitle: DIFUTURE Workshop
author: Daniel Schalk
date: September 5, 2019
output:
  beamer_presentation:
    toc: false
    includes:
      in_header: "style/preamble_reisensburg.sty"
    template: "style/custom_pandoc.tex"
---


# DIFUTURE Workshop 05.09.2019


## The Problem

\addtocounter{framenumber}{-1}

**About the data**:

-   4 Hospitals (we call them clients/sites), each one holds data about patients and a disease
-   For data protection reasons, these data may not be combined

**About the analysis**:

-   A statistician wants to analyze the data and predict whether a patient is sick or not on a single machine (the host)
-   **But:** Most statistical or machine learning approaches require **one** dataset for modeling

\begin{itemize}
  \item[$\Rightarrow$] We want to learn one model on datasets distributed over multiple clients (decentralized learning).
\end{itemize}



```{r, echo=FALSE, results="asis"}
for (image in seq_len(7)) {
  cat(paste0("

## General Concept

", ifelse(image > 1, "\\addtocounter{framenumber}{-1}", ""), "
\\includegraphics[width=\\textwidth, page=", image, "]{images/federated_learning.pdf}

"))
}
```



## Performance Evaluation of Distributed Learning Systems

<!-- Evaluation should be done as close as possible to the real conditions. -->

- To evaluate the performance we usually resample the model \ $\rightarrow$ Not clear how to resample due to the decentralized dataset

-   Possible approaches:
    -   Leave k sites out evaluation
    -   Partitioning of individual datasets:
        -   Split individual datasets and train federated learning model on the individual ones
        -   Subsampling across all sites

What is the data generating process? Is the hospital an important factor (can we account for that)? Do new hospitals want to use the model?

<!-- Werden neue daten auch von neuen KrankenhÃ¤usern geliefert? -->

## Leave k Datasets Out

\begin{center}
  \includegraphics[width=0.5\textwidth, page=1]{images/fed_learn_leave_site_out.pdf}\includegraphics[width=0.5\textwidth, page=5]{images/fed_learn_leave_site_out.pdf}
\end{center}

## Leave k Datasets Out

\addtocounter{framenumber}{-1}
\begin{center}
  \includegraphics[width=0.5\textwidth, page=2]{images/fed_learn_leave_site_out.pdf}\includegraphics[width=0.5\textwidth, page=6]{images/fed_learn_leave_site_out.pdf}
\end{center}

## Leave k Datasets Out

\addtocounter{framenumber}{-1}
\begin{center}
  \includegraphics[width=0.5\textwidth, page=3]{images/fed_learn_leave_site_out.pdf}\includegraphics[width=0.5\textwidth, page=7]{images/fed_learn_leave_site_out.pdf}
\end{center}

## Leave k Datasets Out

\addtocounter{framenumber}{-1}
\begin{center}
  \includegraphics[width=0.5\textwidth, page=4]{images/fed_learn_leave_site_out.pdf}\includegraphics[width=0.5\textwidth, page=8]{images/fed_learn_leave_site_out.pdf}
\end{center}

Problem: It may happen, that sites have a different data distribution, hence the model doesn't get the chance to learn from this distribution and is not able to predict well.



## Partitioning of Individual Datasets

-   **Subsampling**: Randomly sample observation used for training and testing
    \begin{center}
    \includegraphics[width=0.46\textwidth, page=1]{images/fed_learn_resampling.pdf}\includegraphics[width=0.46\textwidth, page=2]{images/fed_learn_resampling.pdf}
    \end{center}

    $\rightarrow$ Not all observations are used for training or testing.



## Partitioning of Individual Datasets

-   **Cross Validation**: Split individual datasets into k pieces
    \begin{center}
    \includegraphics[width=0.46\textwidth, page=1]{images/fed_learn_crossval.pdf}\includegraphics[width=0.46\textwidth, page=2]{images/fed_learn_crossval.pdf}
    \end{center}



## Practical Difficulties

<!-- Points that require input from other parties -->

-   What information is allowed to get shared?

-   No expertise in how to set up and control communication between host and clients:
    -   What are the requirements (Docker?)
    -   How expensive is the communication? Is it better to reduce communication?
    -   What about parallelization?

-   What does the PHT need to fit a model?




## Correcting for Features Shifts

Detecting feature shifts of individual datasets to correct them.

-   Assumption: Distribution of observations of individual datasets is equal

-   Train a surrogate model instead of averaging the updates:
    -   Is it possible to correct the model for these features?
    -   The surrogate model can be used to give insights about problems of individual datasets.

$\rightarrow$ Train model-based boosting model using the proposed federated learning framework.
