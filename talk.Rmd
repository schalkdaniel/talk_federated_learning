---
title: Federated Learning
subtitle: Idea, Applications, and 
author: Daniel Schalk
date: \today
output:
  beamer_presentation:
    toc: true
    includes:
      in_header: "style/preamble_reisensburg.sty"
    template: "style/custom_pandoc.tex"
---



# The Problem

## Use Case

**About the data:**

-   5 Hospitals, each holds data about patients and a disease
-   For data protection reasons, these data may not be combined

**About the analysis:**

-   Now a statistician wants to analyze the data and predict whether a patient is diseased or not
-   **But:** Most statistical or machine learning approaches requires **one** database for modeling

\begin{itemize}
  \item[$\Rightarrow$] We want to learn one model on decentralized datasets distributed over multiple sources.
\end{itemize}

# Federated/Decentralized Learning

## Distributed Learning

Different approaches of parallelization in centralized learning:

-   **Algorithmic:** Learning algorithm naturally supports parallelization, e.g. Random Forest
-   **Data:** Data are distributed over multiple machines (idea behind Hadoop and Spark), e.g. map reduce

Decentralized learning is a combination of both:

-   Data are distributed over multiple machines and the algorithm must be able to learn on the local datasets

## What is it About?

\includegraphics[width=\textwidth,page=1]{images/federated_learning.pdf}

## What is it About?

\includegraphics[width=\textwidth,page=2]{images/federated_learning.pdf}
\addtocounter{framenumber}{-1}

## What is it About?

\includegraphics[width=\textwidth,page=3]{images/federated_learning.pdf}
\addtocounter{framenumber}{-1}

## What is it About?

\includegraphics[width=\textwidth,page=4]{images/federated_learning.pdf}
\addtocounter{framenumber}{-1}

## What is it About?

\includegraphics[width=\textwidth,page=5]{images/federated_learning.pdf}
\addtocounter{framenumber}{-1}

## What is it About?

\includegraphics[width=\textwidth,page=6]{images/federated_learning.pdf}
\addtocounter{framenumber}{-1}

## Common Problems in Decentralized Learning 

Federated Learning \cite{mcmahan2016communication} as learning on decentralized data with the following properties:

-   **Non-IID:** The training data on a given client is typically based on external influences, and hence any clients local dataset will not be representative of the population distribution.

-   **Unbalanced:** Similarly, varying amounts of local training data on each client.

-   **Distributed:** We expect the number of clients participating in an optimization larger than one.

-   **Limited communication:** Clients are frequently offline or on slow or expensive connections $\Rightarrow$ New resource communication.



# Federated Learning

## Terminology

|                      |                                           |
|:-------------------- |:-----------------------------------------:|
| **Feature Vector**   | $x \in \mathcal{X}$                       |
| **Target Variable**  | $y \in \mathcal{Y}$                       |
| **Parameter Vector** | $\theta \in \Theta$                       |
| **Model**            | $f : \mathcal{X} \rightarrow \mathcal{Y}$ |
| **Prediction**       | $\hat{y} = f(x, \hat{\theta})$            |
| **Loss Function**    | $L\left(y, \hat{y}\right)$                |
| **Dataset**          | $\mathcal{D} = (x^{(i)}, y^{(i)}),\ \forall i \in \{1, \dots n\}$ |
| **Empirical Risk**   | $\mathcal{R}_\mathsf{emp}(\mathcal{D}, \hat{\theta}) = \frac{1}{n}\sum\limits_{(x,y) \in \mathcal{D}} L\left(y, f(x,\hat{\theta})\right)$ |

## Gradient Descent

$$\hat{\theta}_{t+1} = \hat{\theta}_t - \eta\nabla_\theta \mathcal{R}_\mathsf{emp}(\hat{\theta}_t)$$

-   With gradient: $$\frac{\delta}{\delta\theta} L(y,f(x,\theta)) = \nabla_\theta \mathcal{R}_\mathsf{emp}(\theta)$$
-   Learning rate $\eta > 0$
-   Initial parameter vector $\hat{\theta}_0$

## Federated Averaging - 1

-   We now got $K$ different clients
-   Each client holds a non-distributable dataset $\mathcal{D}_k$, $k \in \{1, \dots, K\}$ with $n_k$ observations
-   Each dataset yields an empirical risk $\mathcal{R}_\mathsf{emp}(\mathcal{D}_k, \theta)$

\begin{itemize}
  \item[$\Rightarrow$] How to find a good model (represented by $\hat{\theta}$) trained on all datasets?
\end{itemize}

## Federated Averaging - 2

\scriptsize

\begin{Shaded}
\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{$\mathcal{D}_1, \dots, \mathcal{D}_K$}
  \KwResult{Parameter vector $\hat{\theta}$}
  \textbf{Initialization:} $\hat{\theta}_0$ e.g. randomly and set $t = 1$\;
  \While{Stop criteria is not reached}{
    Send $\hat{\theta}_{t-1}$ to all $K$ clients\;
    \For{$k = 1$ \KwTo $K$}{
      Calculate and report $\nabla_\theta \mathcal{R}_\mathsf{emp}(\mathcal{D}_k, \hat{\theta}_{t-1})$ to host\;
    }
    \tcc{Host conduct Federated Averaging step:}
    $\hat{\theta}_{t} = \hat{\theta}_{t-1} - \eta \sum\limits_{k = 1}^K \frac{n_k}{n} \nabla_\theta \mathcal{R}_\mathsf{emp}(\mathcal{D}_k, \hat{\theta}_{t-1})$\;
    Check if stop criteria is reached, e.g. $\|\hat{\theta}_t - \hat{\theta}_{t-1}\|_2 < \varepsilon$\;
    Increment $t \leftarrow t + 1$ \;
  }
\end{algorithm}
\end{Shaded}

\normalsize

This algorithm requires communication between host and clients after each iteration. $\rightarrow$ \alert{Very expensive!}

## Federated Averaging - 3

In order to make one huge update on the host side we can conduct the update on the client side and average the updates:
$$
\gamma_k = \hat{\theta}_{t-1} - \eta \mathcal{R}_\mathsf{emp}(\mathcal{D}_k, \hat{\theta}_{t-1})
$$
$$
\Rightarrow\ \hat{\theta}_t = \hat{\theta}_{t-1} - \eta \sum\limits_{k = 1}^K \frac{n_k}{n} \nabla_\theta \mathcal{R}_\mathsf{emp}(\mathcal{D}_k, \hat{\theta}_{t-1}) = \sum\limits_{k = 1}^K\frac{n_k}{n}\gamma_k
$$
Additionally, we can think of different methods to average the reported updates:
$$
\hat{\theta}_t = \mathsf{avg}(\gamma_1, \dots, \gamma_K, \alpha) = \sum\limits_{k = 1}^K \frac{n_k}{n} \gamma_{k}
$$

## Federated Averaging - 4

\textbf{Host side:} Distribute and collect data

\scriptsize

\begin{Shaded}
\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{$\mathcal{D}_1, \dots, \mathcal{D}_K$}
  \KwResult{Parameter vector $\hat{\theta}$}
  \textbf{Initialization:} $\hat{\theta}_0$ e.g. randomly and set $t = 1$\;
  \While{Stop criteria is not reached}{
    Send $\hat{\theta}_{t-1}$ to all $K$ clients\;
    \For{$k = 1$ \KwTo $K$}{
      $\gamma_k = \mathsf{clientUpdate}(k, \hat{\theta}_{t-1})$\;
      Report $\gamma_{k}$ to host\;
    }
    \tcc{Host conduct Federated Averaging step:}
    $\hat{\theta}_{t} = \mathsf{avg}(\gamma_1, \dots, \gamma_K, \alpha)$\;
    Check if stop criteria is reached, e.g. $\|\hat{\theta}_t - \hat{\theta}_{t-1}\|_2 < \varepsilon$\;
    Increment $t \leftarrow t + 1$ \;
  }
\caption{Federated Averaging Algorithm}\label{fedavg}
\end{algorithm}
\end{Shaded}

\normalsize

## Federated Averaging - 5

With algorithm \ref{fedavg} we can also think about reducing communication costs. Therefore, we conduct $E$ updates in $\mathsf{clientUpdate}$:

\scriptsize
\begin{Shaded}
\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{$\mathcal{D}_k$ and $\hat{\theta}_{t-1}$}
  \KwResult{$k$-th client update $\gamma_k$}
  \textbf{Initialization:} $\gamma_{k,0} = \hat{\theta}_{t-1}$\;
  \For{$i = 1$ \KwTo $E$}{
    $\gamma_{k,i} = \gamma_{k,i-1} - \eta\nabla_\theta \mathcal{R}_\mathsf{emp}(\mathcal{D}_k, \gamma_{k,i-1})$\;
  }
  Report $\gamma_k = \gamma_{k,E}$ to host\;  
\caption{Communication reduction in $\mathsf{clientUpdate}$}
\end{algorithm}
\end{Shaded}
\normalsize

<!-- -   Mention problems and how we can tackle them (more steps in one iteration ...) -->

<!-- # Challenges

## Communication Costs vs. Training Costs

## Evaluation of Federated Learning Systems -->

# Example with Logistic Regression

## Logistic Regression

-   **Loss Function/Negative Log-Likelihood** $$L\left(y, f(x, \theta)\right) = -y \log\left(f(x, \theta)\right) - (1 - y) \log\left(1 - f(x, \theta)\right)$$
-   **Response Function** $$f(x) = \left(1 + \exp\left(-x^T\theta\right)\right)^{-1}$$
-   **Score Function** $$\frac{\delta}{\delta\theta}L(y,f\left(x,\theta)\right) = x \left(y - f(x,\theta)\right)$$

## Simulation Setup - 1

-   Correlations are simulated from beta distribution $r_{ij} = B(1,2)$ for $i \neq j$, $r_{ii} = 1$
-   Dataset is simulated using a multivariate normal distribution $x^{(i)} \sim N_p(\mu, R)$
-   Binary labels are generated by sampling from Bernoulli distribution $B\left(1,f(x^{(i)})\right)$

## Simulation Setup - 2

-   The dataset is split into $K$ "local datasets" under consideration of different data situations:
    -   **Setup 1:** Local datasets are IID
    -   **Setup 2:** One variable of one of the IID datasets gets a bias (e.g. damaged sensor)
    -   **Setup 3:** Unbalanced local datasets
    -   **Setup 4:** Unbalanced and non-IID, but with biases that have a mean of zero

-   We conduct Federated Averaging until the norm of the update $\|\hat{\theta}_t - \hat{\theta}_{t-1}\|_2$ is smaller than $10^{-7}$ for varying learning rates $\eta$ and number of updates at once $E$:
    -   $\eta\in\{0.001, 0.01, 0.05, 0.1\}$
    -   $E\in\{1, 10, 100, 1000, 2000\}$

For visualization purposes we simulate the data without intercept and $p = 2$.

## Visualization Setup 1

\includegraphics[width=\textwidth]{images/fed_logreg_iid.pdf}

## Visualization Setup 2

\includegraphics[width=\textwidth]{images/fed_logreg_set1.pdf}

## Visualization Setup 3

\includegraphics[width=\textwidth]{images/fed_logreg_iid_ub.pdf}

## Visualization Setup 4

\includegraphics[width=\textwidth]{images/fed_logreg_ub_non_iid.pdf}


# Boosting and Federated Learning

## Centralized Approaches

- DistBoost http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.8866&rep=rep1&type=pdf
- DiVote http://www.jmlr.org/papers/volume5/chawla04a/chawla04a.pdf (parallel IVote http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.308.8562&rep=rep1&type=pdf) 
- PreWeak https://stanford.edu/~rezab/nips2014workshop/submits/distboost.pdf

# Further Ideas

## Further Ideas

-   Experiments with different averaging approaches for $\mathsf{avg}(\gamma_1, \dots, \gamma_K, \alpha)$:
    -   Use of other optimizer, such as momentum or adagrad
    -   Reweighting of the updates by applying Bayes theorem
    -   Train $\mathsf{avg}$ as surrogate model during the fitting process 

-   Inherent evaluation on local datasets (useful for early stopping)

# References

##

\thispagestyle{empty}
\begingroup
  \renewcommand{\section}[2]{}
  \scriptsize
  \bibliographystyle{apalike}
  \bibliography{references}
\endgroup